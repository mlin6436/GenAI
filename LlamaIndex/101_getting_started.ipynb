{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e8854b",
   "metadata": {},
   "source": [
    "# Getting Started with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67061789",
   "metadata": {},
   "source": [
    "**Disclaimer**: _LlamaIndex is under active development, you should expect to see a lot of changes, always refer to the code relevant to the version of LlamaIndex you are using for up-to-date information. One example is whilst this instruction is being created, `GPTVectorStoreIndex` is still available in `v0.6.19`, however work to [remove GPT prefix](https://github.com/jerryjliu/llama_index/pull/5822) is currently underway._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0b6b4",
   "metadata": {},
   "source": [
    "LlamaIndex is created to provide an interface between LLMs and your data, to achieve a range of tasks, including `question-answering`, `summarization`, `structured queries` etc.\n",
    "\n",
    "Let's take a look at a flow diagram for LlamaIndex's query interface.\n",
    "\n",
    "![query_interface](query_interface.png)\n",
    "\n",
    "- **Documents**: Data is represented in the form of Document. LlamaIndex provides a range of data connectors to load data from various sources. Refer to [Llama Hub](https://llamahub.ai/), you are most likely to find the data connector you need, of course, feel free to contribute to it if not.\n",
    "- **Nodes**: Nodes are effectively chunks of source documents. They can be chunks of text, images, and more.\n",
    "- **Index**: Index is at the core of LlamaIndex library. It abstracts away underlying storage and maintains the state of the processed documents., and exposes a lightweight view of the data.\n",
    "- **Retriever**: A retriever retrieves the most relevant nodes from an index given a query.\n",
    "- **Response Synthesizer**: A synthesizer can filter and augment the retrieved nodes to further optimise the relevancy of result and reduce token cost.\n",
    "- **Query Engine**: A query engine takes in a user query and returns a response.\n",
    "- **Storage Context**: The storage context provides the abstraction layer over data storage layer.\n",
    "- **Storage**: This can be a number of things, document store (documents), index store (metadata) or vector store (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66365639",
   "metadata": {},
   "source": [
    "To start the tutorial, first install `llama-index` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9918e",
   "metadata": {},
   "source": [
    "Make sure `OpenAI API key` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2f85ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR OPENAI API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891f3c7",
   "metadata": {},
   "source": [
    "Alternatively, if you have OpenAI api key setup in `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb2739",
   "metadata": {},
   "source": [
    "## Load in Documents\n",
    "\n",
    "First step is to load the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "974f8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(text='The Tragedy of Hamlet, Prince of\\nDenmark\\nASCII text placed in the public domain by Moby Lexical Tools, 1992. SGML markup by Jon Bosak,\\n1992-1994. XML version by Jon Bosak, 1996-1999. Simplified XML version by Max Froumentin, 2001. The\\nXML markup in this version is Copyright © 1999 Jon Bosak. This work may freely be distributed on condition\\nthat it not be modified or altered in any way.', doc_id='36ce4a60-be0f-42e3-8f10-62051a88d3ac', embedding=None, doc_hash='dc9a9296c1508709c152f8fe8fd787359df4934ddd203486243327a44b6f92bb', extra_info={'page_label': '1', 'file_name': 'hamlet.pdf'})\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6b860",
   "metadata": {},
   "source": [
    "**Customise a Document**\n",
    "\n",
    "You can also manually create a `Document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5213aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import Document\n",
    "\n",
    "customised_document = Document(\n",
    "    documents[0],\n",
    "    extra_info = {\n",
    "        'filename': 'hamlet.pdf',\n",
    "        'genre': 'drama'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcc465",
   "metadata": {},
   "source": [
    "## Parse the Documents into Nodes\n",
    "\n",
    "Then turn the documents loaded into a list of `nodes`, affectively `chunks` of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e8b811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(text='The Tragedy of Hamlet, Prince of\\nDenmark\\nASCII text placed in the public domain by Moby Lexical Tools, 1992. SGML markup by Jon Bosak,\\n1992-1994. XML version by Jon Bosak, 1996-1999. Simplified XML version by Max Froumentin, 2001. The\\nXML markup in this version is Copyright © 1999 Jon Bosak. This work may freely be distributed on condition\\nthat it not be modified or altered in any way.', doc_id='3b55546b-b1b7-4f46-ba52-61fa7153abcb', embedding=None, doc_hash='dc9a9296c1508709c152f8fe8fd787359df4934ddd203486243327a44b6f92bb', extra_info={'page_label': '1', 'file_name': 'hamlet.pdf'}, node_info={'start': 0, 'end': 388, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '36ce4a60-be0f-42e3-8f10-62051a88d3ac'})\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c57102",
   "metadata": {},
   "source": [
    "## Construct Index\n",
    "\n",
    "`Index` is the interface that combines the data (from nodes or documents) and LLM, and make the data ready for querying.\n",
    "\n",
    "There are many different types of indexes, such as `List Index`, `Vector Store Index`, `Tree Index`, `Keyword Table Index`, `Structured Store Index` and `Knowledge Graph Index`. All created for unique use cases. Here, I will only focus on the most commonly used indexes, and elaborate on other index types another time.\n",
    "\n",
    "This is the simplest way to create an index from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed2a134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922c9b9",
   "metadata": {},
   "source": [
    "**Reuse Nodes**\n",
    "\n",
    "You can also create an `index` from the nodes created. Also note that once the nodes are created and persisted, they can be reused as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf75dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, GPTListIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "index1 = GPTVectorStoreIndex(nodes, storage_context = storage_context)\n",
    "index2 = GPTListIndex(nodes, storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecabb4",
   "metadata": {},
   "source": [
    "**Insert Nodes or Documents**\n",
    "\n",
    "To enrich index with more data, you can insert more nodes or documents to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dfcce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex([])\n",
    "index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "002190d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex([])\n",
    "for document in documents:\n",
    "    index.insert(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576ba0a",
   "metadata": {},
   "source": [
    "**Customise LLMs**\n",
    "\n",
    "You can also customise the LLMs used in the index. The default model is `text-davinci-003`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7e8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, ServiceContext\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003')\n",
    "llm_predictor = LLMPredictor(llm = llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor = llm_predictor)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context = service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee42662",
   "metadata": {},
   "source": [
    "**Persist Index**\n",
    "\n",
    "Index can also be persisted via `Storage Context` so that it can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc7f2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "index.storage_context.persist(persist_dir = 'storage')\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir = 'storage')\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58dfab",
   "metadata": {},
   "source": [
    "## Query Index\n",
    "\n",
    "In order to query the index, first turn an `index` into a `query engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "467883f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to Hamlet, what keeps a person from escaping the troubles of this life is the fear of the unknown after death. He states that the \"dread of something after death, the undiscover'd country from whose bourn no traveller returns, puzzles the will and makes us rather bear those ills we have than fly to others that we know not of.\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What, according to Hamlet, keeps a person from escaping the troubles of this life?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1116c",
   "metadata": {},
   "source": [
    "**Custom Index**\n",
    "\n",
    "However, query engine creation can be more sophisticated. You can specify a `retriever` and `response synthesizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea67e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hamlet is prompted to say, \"My thoughts be bloody or nothing worth!\" after he reflects on the imminent death of twenty thousand men who are going to their graves for a plot of land that is not worth fighting for. He is overwhelmed by the senselessness of the situation and is determined to take revenge for the injustice.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ResponseSynthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What prompts Hamlet to say, 'My thoughts be bloody or nothing worth!'?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3919c75c",
   "metadata": {},
   "source": [
    "**Configure Retriever**\n",
    "\n",
    "There are a couple of ways to define a retriever. Followings are the second, where you convert the index into a retriever by supplying the `mode` of retriever.\n",
    "- **default**: This creates a simple retriever, `ListIndexRetriever`, for `ListIndex` that returns all nodes.\n",
    "- **embedding**: This creates an embedding based retriever, `ListIndexEmbeddingRetriever`, for `ListIndex`.\n",
    "- **llm**: This creates an LLM based retriever, `ListIndexLLMRetriever`, for `ListIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65755a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to Hamlet, the fear of something after death, the unknown of the \"undiscovered country from whose bourn no traveller returns,\" keeps a person from escaping the troubles of this life.\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(retriever_mode='default')\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "response = query_engine.query(\"What, according to Hamlet, keeps a person from escaping the troubles of this life?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4207b5",
   "metadata": {},
   "source": [
    "**Configure Response Synthesis**\n",
    "\n",
    "Further more, you can also customise `response_synthesizer`. Here is another way to define the `mode` of response_synthesizer. There are currently eight different modes.\n",
    "- **refine**: This uses the context in the first node, along with the query, to generate an initial answer. Then pass this answer, the query, and the context of the second node as input into a \"refine prompt\" to generate a refined answer.\n",
    "- **compact**: This is the default mode. It first combines text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM.\n",
    "- **simple_summarize**: It merges all text chunks into one, and make a LLM call. This will fail if the merged text chunk exceeds the context window size.\n",
    "- **tree_summarize**: This approach builds a tree index over the set of candidate nodes, with a summary prompt seeded with the query. \n",
    "- **generation**: It ignores context, just use LLM to generate a response.\n",
    "- **no_text**: It returns the retrieved context nodes, without synthesizing a final response.\n",
    "- **accumulate**: It synthesises a response for each text chunk, and then return the concatenation.\n",
    "- **compact_accumulate**: This mode combines text chunks into larger consolidated chunks that more fully utilise the available context window, then accumulate answers for each of them and finally return the concatenation. This mode is faster than accumulate since we make fewer calls to the LLM.\n",
    "\n",
    "I will list a few representative modes for your reference below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0b9d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_mode='compact': \n",
      "Hamlet is prompted to say, \"My thoughts be bloody or nothing worth!\" after reflecting on the imminent death of twenty thousand men who are fighting for a plot of land that is not worth the cost of their lives. He is disgusted by the idea of so many people dying for something so trivial and decides that his thoughts must be bloody in order to be of any worth.\n",
      "\n",
      "response_mode='tree_summarize': \n",
      "Hamlet is prompted to say, \"My thoughts be bloody or nothing worth!\" after reflecting on the imminent death of twenty thousand men who are fighting for a plot of land that is not worth the cost of their lives. He is disgusted by the idea of so many people dying for something so trivial and decides that his thoughts must be bloody in order to be of any worth.\n",
      "\n",
      "response_mode='generation': \n",
      "\n",
      "Hamlet says this line in Act IV, Scene IV, after he has decided to take revenge on his uncle Claudius for murdering his father. He is reflecting on his decision and the consequences of it, and he realizes that his thoughts must be focused on revenge or they will be worthless.\n",
      "\n",
      "response_mode='compact_accumulate': Response 1: \n",
      "Hamlet is prompted to say, \"My thoughts be bloody or nothing worth!\" after reflecting on the imminent death of twenty thousand men who are fighting for a cause that cannot be tried by numbers. He is overwhelmed by the senselessness of the situation and is determined to take revenge for his father's death.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')\n",
    "response = query_engine.query(\"What prompts Hamlet to say, 'My thoughts be bloody or nothing worth!'?\")\n",
    "print(f\"response_mode='compact': {response}\\n\")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\n",
    "response = query_engine.query(\"What prompts Hamlet to say, 'My thoughts be bloody or nothing worth!'?\")\n",
    "print(f\"response_mode='tree_summarize': {response}\\n\")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='generation')\n",
    "response = query_engine.query(\"What prompts Hamlet to say, 'My thoughts be bloody or nothing worth!'?\")\n",
    "print(f\"response_mode='generation': {response}\\n\")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact_accumulate')\n",
    "response = query_engine.query(\"What prompts Hamlet to say, 'My thoughts be bloody or nothing worth!'?\")\n",
    "print(f\"response_mode='compact_accumulate': {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bec2a8",
   "metadata": {},
   "source": [
    "**Configure Node Post-Processors**\n",
    "\n",
    "The `post-processors` work via `filtering` and `augmenting` retrieved nodes. This is a more advanced feature, it can improve the relevancy of the the retrieved nodes, and reduce the time and number of calls to LLMs. To demonstrate its usage, let's compare the results for this:\n",
    "\n",
    "**Question**: What is the content of Hamlet's letter to Horatio?\n",
    "\n",
    "**Answer**: It explains he escaped Rosencrantz and Guildenstern onto a Pirate ship. They are treating him well because pirates want favors from him. He wants Horatio to give letters to King and Queen and come to see him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c737e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hamlet's letter is to his friend Horatio, informing him that Rosencrantz and Guildenstern are on their way to England and that he has something important to tell him. He also expresses his grief over his mother's remarriage to his uncle so soon after his father's death. He laments the frailty of women and the unfairness of the world.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the content of Hamlet's letter?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2861e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hamlet's letter is to a friend, instructing them to bring Rosencrantz and Guildenstern to him with as much speed as possible. He has words to tell them that will make them dumb, and he promises to tell them more when they arrive. He also asks his friend to deliver the letters he has sent to the King.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.postprocessor.node import KeywordNodePostprocessor\n",
    "\n",
    "node_postprocessors = [\n",
    "    KeywordNodePostprocessor(\n",
    "        required_keywords=[\"Rosencrantz\", \"Guildenstern\"]\n",
    "    )\n",
    "]\n",
    "query_engine = index.as_query_engine(node_postprocessors=node_postprocessors)\n",
    "response = query_engine.query(\"What is the content of Hamlet's letter?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9ad2f",
   "metadata": {},
   "source": [
    "## Parse the Response\n",
    "\n",
    "Before the result is presented to the users, you can format the `Response` object accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a239884",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What, according to Hamlet, keeps a person from escaping the troubles of this life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e01526ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "According to Hamlet, the fear of something after death, the unknown of the \"undiscovered country from whose bourn no traveller returns,\" keeps a person from escaping the troubles of this life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f9248ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: d9c09b96-8dd9-4721-841b-82c05aee7dad): page_label: 63\n",
      "file_name: hamlet.pdf\n",
      "\n",
      "Your loneliness. We are oft to blame in this,--\n",
      "'Tis too mu...\n",
      "\n",
      "> Source (Doc id: 65ddf2d1-501b-4a70-8c80-608ee84c9e1f): page_label: 49\n",
      "file_name: hamlet.pdf\n",
      "\n",
      "HAMLET\n",
      "Then is doomsday near: but your news is not true.\n",
      "Le...\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d61663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=Node(text=\"Your loneliness. We are oft to blame in this,--\\n'Tis too much proved--that with devotion's visage\\nAnd pious action we do sugar o'er\\nThe devil himself.\\nKING CLAUDIUS\\nAside\\nO, 'tis too true!\\nHow smart a lash that speech doth give my conscience!\\nThe harlot's cheek, beautied with plastering art,\\nIs not more ugly to the thing that helps it\\nThan is my deed to my most painted word:\\nO heavy burthen!\\nLORD POLONIUS\\nI hear him coming: let's withdraw, my lord.\\nExeunt KING CLAUDIUS and POLONIUS\\nEnter HAMLET\\nHAMLET\\nTo be, or not to be: that is the question:\\nWhether 'tis nobler in the mind to suffer\\nThe slings and arrows of outrageous fortune,\\nOr to take arms against a sea of troubles,\\nAnd by opposing end them? To die: to sleep;\\nNo more; and by a sleep to say we end\\nThe heart-ache and the thousand natural shocks\\nThat flesh is heir to, 'tis a consummation\\nDevoutly to be wish'd. To die, to sleep;\\nTo sleep: perchance to dream: ay, there's the rub;\\nFor in that sleep of death what dreams may come\\nWhen we have shuffled off this mortal coil,\\nMust give us pause: there's the respect\\nThat makes calamity of so long life;\\nFor who would bear the whips and scorns of time,\\nThe oppressor's wrong, the proud man's contumely,\\nThe pangs of despised love, the law's delay,\\nThe insolence of office and the spurns\\nThat patient merit of the unworthy takes,\\nWhen he himself might his quietus make\\nWith a bare bodkin? who would fardels bear,\\nTo grunt and sweat under a weary life,\\nBut that the dread of something after death,\\nThe undiscover'd country from whose bourn\\nNo traveller returns, puzzles the will\\nAnd makes us rather bear those ills we have\\nThan fly to others that we know not of?\\nThus conscience does make cowards of us all;\\nAnd thus the native hue of resolutionHAMLET - Act III\\n63\", doc_id='d9c09b96-8dd9-4721-841b-82c05aee7dad', embedding=None, doc_hash='9fa21d5e4dc0e4d9d3da9f12f5cedac645c5a7f6d442baff064d1434f6d1560b', extra_info={'page_label': '63', 'file_name': 'hamlet.pdf'}, node_info={'start': 0, 'end': 1771, '_node_type': '1'}, relationships={<DocumentRelationship.SOURCE: '1'>: '4261f4de-ee69-4871-bd62-41e6c887442f'}), score=0.8514017549553331), NodeWithScore(node=Node(text=\"HAMLET\\nThen is doomsday near: but your news is not true.\\nLet me question more in particular: what have you,\\nmy good friends, deserved at the hands of fortune,\\nthat she sends you to prison hither?\\nGUILDENSTERN\\nPrison, my lord!\\nHAMLET\\nDenmark's a prison.\\nROSENCRANTZ\\nThen is the world one.\\nHAMLET\\nA goodly one; in which there are many confines,\\nwards and dungeons, Denmark being one o' the worst.\\nROSENCRANTZ\\nWe think not so, my lord.\\nHAMLET\\nWhy, then, 'tis none to you; for there is nothing\\neither good or bad, but thinking makes it so: to me\\nit is a prison.\\nROSENCRANTZ\\nWhy then, your ambition makes it one; 'tis too\\nnarrow for your mind.\\nHAMLET\\nO God, I could be bounded in a nut shell and count\\nmyself a king of infinite space, were it not that I\\nhave bad dreams.\\nGUILDENSTERN\\nWhich dreams indeed are ambition, for the very\\nsubstance of the ambitious is merely the shadow of a dream.\\nHAMLET\\nA dream itself is but a shadow.\\nROSENCRANTZ\\nTruly, and I hold ambition of so airy and light a\\nquality that it is but a shadow's shadow.\\nHAMLET\\nThen are our beggars bodies, and our monarchs andHAMLET - Act II\\n49\", doc_id='65ddf2d1-501b-4a70-8c80-608ee84c9e1f', embedding=None, doc_hash='bd58ffcc11f442af6f9ade89e5cc7972be5587d116f18d5deee053b492933f28', extra_info={'page_label': '49', 'file_name': 'hamlet.pdf'}, node_info={'start': 0, 'end': 1103, '_node_type': '1'}, relationships={<DocumentRelationship.SOURCE: '1'>: '368458ca-8017-41e3-92af-8180e1b6fd64'}), score=0.8450789361821318)]\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
