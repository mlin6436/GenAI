{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0469441b",
   "metadata": {},
   "source": [
    "# LangChain Use Cases\n",
    "\n",
    "This document dives stragith into implementing [LangChain use case](https://docs.langchain.com/docs/category/use-cases) using a step-by-step approach, and hopefully inspire you to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6f639",
   "metadata": {},
   "source": [
    "## Use Case - Summarization\n",
    "\n",
    "Langchain summarization is one of the most common usage of LangChain and LLMs. It can summarize any amount of text or documentations, including the ones that exceeds the token limit (currently set at `4096 tokens` for `gpt-35-turbo`), which roughly equates to `3000 words`. \n",
    "\n",
    "The technique used is similar to the concept of `sliding window`, which a fixed size window, usually under the max context window limit, is used to chunk the the long document into smaller pieces, then summarize the content recursively to produce the final summary using mapreduce.\n",
    "\n",
    "You can use it to summarize not only text, books, documents, audios, social media threads, and etc. Some use cases including: _articles_, _research pagers_, _legal and financial documents_, _transcripts_, _chat history_, _customer interactions_, _product reviews_, _podcasts_, _twitter threads_ and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767ad9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pdfplumber\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccb69b",
   "metadata": {},
   "source": [
    "Now, let's load in `The Tragedy of Hamlet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d234c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedy of Hamlet, Prince of\n",
      "Denmark\n",
      "ASCIItextplacedinthepublicdomainbyMobyLexicalTools,1992.SGMLmarkupbyJonBosak,\n",
      "1992-1994.XMLversionbyJonBosak,1996-1999.SimplifiedXMLversionbyMaxFroumentin,2001.The\n",
      "XMLmarkupinthisversionisCopyrightÂ©1999JonBosak.Thisworkmayfreelybedistributedoncondition\n",
      "thatit\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(\"hamlet.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "        \n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bb1b0",
   "metadata": {},
   "source": [
    "By outputting the total number of tokens of the book with [get_num_tokens](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI.get_num_tokens), you can clearly see it exceeds the context window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a27316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens: 53929\n"
     ]
    }
   ],
   "source": [
    "n_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print(f\"total number of tokens: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbf8a4",
   "metadata": {},
   "source": [
    "The process starts by `splitting` the text into multiple parts by `chunk size` using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html), with a `coverlap` of each adjacent chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c744760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 100\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=200, length_function = len)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(f\"number of docs: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362eb6",
   "metadata": {},
   "source": [
    "Now the docs are ready, we can then load up a `chain` to do produce the sum of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ada059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In The Tragedy of Hamlet, Prince of Denmark, Hamlet is on a quest for revenge against his uncle, Claudius, who has usurped the throne. After encountering a ghostly figure, Hamlet learns that his father was murdered by his uncle and is determined to avenge his death. King Claudius and Lord Polonius devise a plan to spy on Hamlet and Ophelia, and Hamlet kills Polonius. Laertes returns from France and challenges Hamlet to a fencing match, during which Laertes wounds Hamlet with a poisoned sword. Hamlet stabs King Claudius with a poisoned sword and forces him to drink a poisoned potion. Prince Fortinbras arrives and orders that the bodies be placed on a stage for all to see, and then claims his rights of memory in the kingdom.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm = llm, chain_type = 'map_reduce')\n",
    "result = chain.run(docs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34599ae",
   "metadata": {},
   "source": [
    "When instanciating the `chain`, you can also add `Verbose = True` to see the steps LangChain took before producing the final summary, i.e. generating a summary per each chunk of text, and produce the final summary by aggregating the 100 summaries with map reduce.\n",
    "\n",
    "This is a powerful technique to overcome the token limitation. Even though more powerful models will be released in the future to support more tokens, such as Claude now supports [100K tokens](https://www.anthropic.com/index/100k-context-windows) as of 11 May 2023, biggest of the its kind. There is not guaranteed better performance, accuracy or cost-effectiveness.\n",
    "\n",
    "Moreover, you can [parallelise the calls](https://github.com/hwchase17/langchain/issues/1073) to super charge summarization using `batch_size` when instanciating LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801325d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
