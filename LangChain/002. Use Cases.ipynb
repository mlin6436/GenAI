{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0469441b",
   "metadata": {},
   "source": [
    "# LangChain Use Cases\n",
    "\n",
    "This document dives straight into implementing [LangChain use case](https://docs.langchain.com/docs/category/use-cases) using a step-by-step approach, and hopefully inspire you to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6f639",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Langchain summarization is one of the most common usage of LangChain and LLMs. It can summarize any amount of text or documentations, including the ones that exceeds the token limit (currently set at `4096 tokens` for `gpt-35-turbo`), which roughly equates to `3000 words`. \n",
    "\n",
    "The technique used is similar to the concept of `sliding window`, which a fixed size window, usually under the max context window limit, is used to chunk the the long document into smaller pieces, then summarize the content recursively to produce the final summary using mapreduce.\n",
    "\n",
    "You can use it to summarize not only text, books, documents, audios, social media threads, and etc. Some use cases including: _articles_, _research pagers_, _legal and financial documents_, _transcripts_, _chat history_, _customer interactions_, _product reviews_, _podcasts_, _twitter threads_ and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767ad9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pdfplumber\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccb69b",
   "metadata": {},
   "source": [
    "Now, let's load up `The Tragedy of Hamlet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d234c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedy of Hamlet, Prince of\n",
      "Denmark\n",
      "ASCIItextplacedinthepublicdomainbyMobyLexicalTools,1992.SGMLmarkupbyJonBosak,\n",
      "1992-1994.XMLversionbyJonBosak,1996-1999.SimplifiedXMLversionbyMaxFroumentin,2001.The\n",
      "XMLmarkupinthisversionisCopyrightÂ©1999JonBosak.Thisworkmayfreelybedistributedoncondition\n",
      "thatit\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(\"hamlet.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "        \n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bb1b0",
   "metadata": {},
   "source": [
    "By outputting the total number of tokens of the book with [get_num_tokens](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI.get_num_tokens), you can clearly see it exceeds the context window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a27316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens: 53929\n"
     ]
    }
   ],
   "source": [
    "n_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print(f\"total number of tokens: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbf8a4",
   "metadata": {},
   "source": [
    "The process starts by `splitting` the text into multiple parts by `chunk size` using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html), with a `coverlap` of each adjacent chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c744760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 100\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=200, length_function = len)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(f\"number of docs: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362eb6",
   "metadata": {},
   "source": [
    "Now the docs are ready, we can then load up a `chain` to do produce the sum of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ada059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In The Tragedy of Hamlet, Prince of Denmark, Hamlet is on a quest for revenge against his uncle, Claudius, who has usurped the throne. After encountering a ghostly figure, Hamlet learns that his father was murdered by his uncle and is determined to avenge his death. King Claudius and Lord Polonius devise a plan to spy on Hamlet and Ophelia, and Hamlet kills Polonius. Laertes returns from France and challenges Hamlet to a fencing match, during which Laertes wounds Hamlet with a poisoned sword. Hamlet stabs King Claudius with a poisoned sword and forces him to drink a poisoned potion. Prince Fortinbras arrives and orders that the bodies be placed on a stage for all to see, and then claims his rights of memory in the kingdom.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm = llm, chain_type = 'map_reduce')\n",
    "result = chain.run(docs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34599ae",
   "metadata": {},
   "source": [
    "When instanciating the `chain`, you can also add `Verbose = True` to see the steps LangChain took before producing the final summary, i.e. generating a summary per each chunk of text, and produce the final summary by aggregating the 100 summaries with map reduce.\n",
    "\n",
    "This is a powerful technique to overcome the token limitation. Even though more powerful models will be released in the future to support more tokens, such as Claude now supports [100K tokens](https://www.anthropic.com/index/100k-context-windows) as of 11 May 2023, biggest of the its kind. There is not guaranteed better performance, accuracy or cost-effectiveness.\n",
    "\n",
    "Moreover, you can [parallelise the calls](https://github.com/hwchase17/langchain/issues/1073) to super charge summarization using `batch_size` when instanciating LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cb491",
   "metadata": {},
   "source": [
    "## Question Answering Over Documents\n",
    "\n",
    "Similiar to how humans answer questions, first you need to provide some context to LLMs. The context can be in many different formats, such as text documents, SQL database, APIs, etc. For the purpose of this exercise, we will focus on dealing with text documents input as context. \n",
    "\n",
    "However, as the context grows, it naturally exceeds the token limit (again, just like in the case of summarization), and it gets harder and harder to give accurate answer quickly. The problem is solved by converting the context and your question into [embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings), basically a list of vector value representaiton of information; then find out a few results that are closely related to your question before giving out a final answer. The process to do the comparison is via an algorithm called [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). If you are getting confused by the terminologies, don't worry. The formular is as following: \n",
    "\n",
    "`llm(context + question) = your answer`.\n",
    "\n",
    "With this capability, you can _chat with your documents_, _ask questions to papers_, _create study guides_, _reference medical information_ and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899592dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df7bc1",
   "metadata": {},
   "source": [
    "Now, let's load `The Tragedy of Hamlet` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95ff1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages in the doc: 142\n",
      "number of characters in the doc: 179843\n"
     ]
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(\"hamlet.pdf\")\n",
    "doc = loader.load()\n",
    "\n",
    "print(f\"number of pages in the doc: {len(doc)}\")\n",
    "total_chars = sum([len(page.page_content) for page in doc])\n",
    "print(f\"number of characters in the doc: {total_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f59cd5",
   "metadata": {},
   "source": [
    "Now, we can split the PDF into chunks based on our definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7818b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of splitted pages: 143\n",
      "average number of characters in each page: 1,258\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "print(f\"number of splitted pages: {len(docs)}\")\n",
    "print(f\"average number of characters in each page: {total_chars / len(docs):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbdfff",
   "metadata": {},
   "source": [
    "Then convert the documents into embeddings by calling OpenAI API Embedding engine, and store the result in a local vector store called `FAISS`. You can choose from a number of supported [vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) (local or remote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb8f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "knowledge_base = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe70cd",
   "metadata": {},
   "source": [
    "Create a retrieval QA engine to perform queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3324ae93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe subject of Hamlet\\'s second soliloquy, the famous \"To be or not to be\" speech, is the contemplation of life and death and the decision of whether to take action against the troubles of life or to accept them. Hamlet is reflecting on the consequences of his inaction in the face of his father\\'s death and his mother\\'s remarriage, and is considering the idea of suicide as a way to escape his suffering. He is also questioning the value of life and death, and whether it is better to endure the pain of life or to end it. He is further considering the implications of his actions, and whether his life is worth the risk of taking action against his troubles. He is also questioning the power of death, and whether it is better to accept death or to fight against it. Additionally, Hamlet is questioning the nature of ambition and the idea of being \"bounded in a nut shell\" and whether it is possible to be content with one\\'s life despite the troubles and suffering that come with it.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"refine\", retriever = knowledge_base.as_retriever())\n",
    "query = \"\"\"What is the subject of Hamlet's second soliloquy, the famous \"To be or not to be\" speech?\"\"\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1414782",
   "metadata": {},
   "source": [
    "By now, you should have a working example of QA over documents. You should also notice there are a number of similarities comparing to `summarization`, such as the way the context was loaded, splitted, before passing onto the `chain`.\n",
    "\n",
    "You may also have noticed different `chain types` specification used, and there is a good reason for that. Before explaining further, understand that LangChain chain types are basically different ways you can connect LLMs. Here are the supported chain types:\n",
    "- `map_reduce`: It separates texts into batches, feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch.\n",
    "- `map_rerank`: It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.\n",
    "- `refine`: It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches.\n",
    "- `stuff`: The default chain type that uses ALL of the text from the documents in the prompt.\n",
    "\n",
    "Because the chain type will actually affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc092da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
