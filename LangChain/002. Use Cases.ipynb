{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0469441b",
   "metadata": {},
   "source": [
    "# LangChain Use Cases\n",
    "\n",
    "This document dives straight into implementing [LangChain use case](https://docs.langchain.com/docs/category/use-cases) using a step-by-step approach, and hopefully inspire you to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6f639",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Langchain summarization is one of the most common usage of LangChain and LLMs. It can summarize any amount of text or documentations, including the ones that exceeds the token limit (currently set at `4096 tokens` for `gpt-35-turbo`), which roughly equates to `3000 words`. \n",
    "\n",
    "The technique used is similar to the concept of `sliding window`, which a fixed size window, usually under the max context window limit, is used to chunk the the long document into smaller pieces, then summarize the content recursively to produce the final summary using mapreduce.\n",
    "\n",
    "You can use it to summarize not only text, books, documents, audios, social media threads, and etc. Some use cases including: _articles_, _research pagers_, _legal and financial documents_, _transcripts_, _chat history_, _customer interactions_, _product reviews_, _podcasts_, _twitter threads_ and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767ad9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pdfplumber\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccb69b",
   "metadata": {},
   "source": [
    "Now, let's load up `The Tragedy of Hamlet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d234c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedy of Hamlet, Prince of\n",
      "Denmark\n",
      "ASCIItextplacedinthepublicdomainbyMobyLexicalTools,1992.SGMLmarkupbyJonBosak,\n",
      "1992-1994.XMLversionbyJonBosak,1996-1999.SimplifiedXMLversionbyMaxFroumentin,2001.The\n",
      "XMLmarkupinthisversionisCopyrightÂ©1999JonBosak.Thisworkmayfreelybedistributedoncondition\n",
      "thatit\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(\"hamlet.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "        \n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bb1b0",
   "metadata": {},
   "source": [
    "By outputting the total number of tokens of the book with [get_num_tokens](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI.get_num_tokens), you can clearly see it exceeds the context window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a27316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens: 53929\n"
     ]
    }
   ],
   "source": [
    "n_tokens = llm.get_num_tokens(text)\n",
    "\n",
    "print(f\"total number of tokens: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbf8a4",
   "metadata": {},
   "source": [
    "The process starts by `splitting` the text into multiple parts by `chunk size` using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html), with a `coverlap` of each adjacent chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c744760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 100\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=200, length_function = len)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(f\"number of docs: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362eb6",
   "metadata": {},
   "source": [
    "Now the docs are ready, we can then load up a `chain` to do produce the sum of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ada059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In The Tragedy of Hamlet, Prince of Denmark, Hamlet is on a quest for revenge against his uncle, Claudius, who has usurped the throne. After encountering a ghostly figure, Hamlet learns that his father was murdered by his uncle and is determined to avenge his death. King Claudius and Lord Polonius devise a plan to spy on Hamlet and Ophelia, and Hamlet kills Polonius. Laertes returns from France and challenges Hamlet to a fencing match, during which Laertes wounds Hamlet with a poisoned sword. Hamlet stabs King Claudius with a poisoned sword and forces him to drink a poisoned potion. Prince Fortinbras arrives and orders that the bodies be placed on a stage for all to see, and then claims his rights of memory in the kingdom.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm = llm, chain_type = 'map_reduce')\n",
    "result = chain.run(docs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34599ae",
   "metadata": {},
   "source": [
    "When instanciating the `chain`, you can also add `Verbose = True` to see the steps LangChain took before producing the final summary, i.e. generating a summary per each chunk of text, and produce the final summary by aggregating the 100 summaries with map reduce.\n",
    "\n",
    "This is a powerful technique to overcome the token limitation. Even though more powerful models will be released in the future to support more tokens, such as Claude now supports [100K tokens](https://www.anthropic.com/index/100k-context-windows) as of 11 May 2023, biggest of the its kind. There is not guaranteed better performance, accuracy or cost-effectiveness.\n",
    "\n",
    "Moreover, you can [parallelise the calls](https://github.com/hwchase17/langchain/issues/1073) to super charge summarization using `batch_size` when instanciating LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fd670",
   "metadata": {},
   "source": [
    "## Question Answering Over Documents\n",
    "\n",
    "Similiar to how humans answer questions, first you need to provide some context to LLMs. The context can be in many different formats, such as text documents, SQL database, APIs, etc. For the purpose of this exercise, we will focus on dealing with text documents input as context. \n",
    "\n",
    "However, as the context grows, it naturally exceeds the token limit (again, just like in the case of summarization), and it gets harder and harder to give accurate answer quickly. The problem is solved by converting the context and your question into [embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings), basically a list of vector value representaiton of information; then find out a few results that are closely related to your question before giving out a final answer. The process to do the comparison is via an algorithm called [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). If you are getting confused by the terminologies, don't worry. The formular is as following: \n",
    "\n",
    "`llm(context + question) = your answer`.\n",
    "\n",
    "With this capability, you can _chat with your documents_, _ask questions to papers_, _create study guides_, _reference medical information_ and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58bfebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4234d0",
   "metadata": {},
   "source": [
    "Now, let's load `The Tragedy of Hamlet` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1957415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages in the doc: 142\n",
      "number of characters in the doc: 179843\n"
     ]
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(\"hamlet.pdf\")\n",
    "doc = loader.load()\n",
    "\n",
    "print(f\"number of pages in the doc: {len(doc)}\")\n",
    "total_chars = sum([len(page.page_content) for page in doc])\n",
    "print(f\"number of characters in the doc: {total_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497c729",
   "metadata": {},
   "source": [
    "Now, we can split the PDF into chunks based on our definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4680b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of splitted pages: 143\n",
      "average number of characters in each page: 1,258\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "print(f\"number of splitted pages: {len(docs)}\")\n",
    "print(f\"average number of characters in each page: {total_chars / len(docs):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ef387",
   "metadata": {},
   "source": [
    "Then convert the documents into embeddings by calling OpenAI API Embedding engine, and store the result in a local vector store called `FAISS`. You can choose from a number of supported [vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) (local or remote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf8d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "knowledge_base = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bd096",
   "metadata": {},
   "source": [
    "Create a retrieval QA engine to perform queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8cf320eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe subject of Hamlet\\'s second soliloquy, the famous \"To be or not to be\" speech, is the contemplation of life and death and the decision of whether to take action against the troubles of life or to accept them. Hamlet is reflecting on the consequences of his inaction in the face of his father\\'s death and his mother\\'s remarriage, and is considering the idea of suicide as a way to escape his suffering. He is also questioning the value of life and death, and whether it is better to endure the pain of life or to end it. He is further considering the implications of his actions, and whether his life is worth the risk of taking action against his troubles. He is also questioning the power of death, and whether it is better to accept death or to fight against it. Additionally, Hamlet is questioning the nature of ambition and the idea of being \"bounded in a nut shell\" and whether it is possible to be content with one\\'s life despite the troubles and suffering that come with it.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"refine\", retriever = knowledge_base.as_retriever())\n",
    "query = \"\"\"What is the subject of Hamlet's second soliloquy, the famous \"To be or not to be\" speech?\"\"\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17aadf4",
   "metadata": {},
   "source": [
    "By now, you should have a working example of QA over documents. You should also notice there are a number of similarities comparing to `summarization`, such as the way the context was loaded, splitted, before passing onto the `chain`.\n",
    "\n",
    "You may also have noticed different `chain types` specification used, and there is a good reason for that. Before explaining further, understand that LangChain chain types are basically different ways you can connect LLMs. Here are the supported chain types:\n",
    "- `map_reduce`: It separates texts into batches, feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch.\n",
    "- `map_rerank`: It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.\n",
    "- `refine`: It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches.\n",
    "- `stuff`: The default chain type that uses ALL of the text from the documents in the prompt.\n",
    "\n",
    "Because the chain type will actually affect the result, I highly recommend you try different types, and figure out which works best for your scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2d3d",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "LLMs are very good at extracting structured information out of unstructured text, whehter that's extracting one or multiple stuctured row from text to insert into database from a sentence, or extracting the correct API params from a user query.\n",
    "\n",
    "In essence, it is about working with [OutputParsers](https://python.langchain.com/en/latest/modules/prompts/output_parsers.html), which is responsible for specifying the schema a LLM should respond in, and then parsing their raw-text output into that structured format.\n",
    "\n",
    "Without further ado, let's dive straight into an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9e72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature = 0, model = \"gpt-3.5-turbo\", openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16436c0a",
   "metadata": {},
   "source": [
    "This step is to define the schema, or more specicially, format instructions, to be added to the prompt, so that LLMs can act accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "326f8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"author\": string  // The name of the author\n",
      "\t\"books\": string  // The books published\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"author\", description=\"The name of the author\"),\n",
    "    ResponseSchema(name=\"books\", description=\"The books published\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98ced9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Extract the author and books.\\n\\n{format_instructions}\\n{user_prompt}\"\"\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d199b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the author and books.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"author\": string  // The name of the author\n",
      "\t\"books\": string  // The books published\n",
      "}\n",
      "```\n",
      "I recently read the book The Tragedy of Hamlet written by William Shakespeare\n"
     ]
    }
   ],
   "source": [
    "query = prompt.format_prompt(user_prompt=\"I recently read the book The Tragedy of Hamlet written by William Shakespeare\")\n",
    "print (query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3b67499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'William Shakespeare', 'books': 'The Tragedy of Hamlet'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output = chat_model(query.to_messages())\n",
    "result = output_parser.parse(output.content)\n",
    "\n",
    "print (result)\n",
    "print (type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f696e",
   "metadata": {},
   "source": [
    "This is pretty cool example of data extraction and sanitisation, however, it is also quite basic. In real life scenario, you are likely to be dealing with more complex data and schema, such as nested structures. In which case, make sure to check out [Kor](https://eyurtsev.github.io/kor/), which provides the next level functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c60c22",
   "metadata": {},
   "source": [
    "## Querying Tabular Data\n",
    "\n",
    "Aside from unstructured data, there is also a lot of structured data sitting in CSVs, SQL, whether that's financial data, report or models or gazillions of day-to-day data that every company has. LLMs can deal with that as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f841ed7",
   "metadata": {},
   "source": [
    "It requires a bit of prep in order to demonstrate this:\n",
    "- To start with, I'll be using a `SQLite` database which basically stores `Goodreads Top 100 Classics`, this data comes from [kaggle](https://www.kaggle.com/datasets/notkrishna/goodreads-top-100-classical-books-of-all-time). Make sure you have SQLite installed or run `brew install sqlite3`.\n",
    "- Use [this](https://www.convertcsv.com/csv-to-sql.htm) to convert CSV data into a SQL INSERT script, results are kept in `goodreads.sql`.\n",
    "- Run the command to insert the data into a database called `goodreads.db`.\n",
    "```\n",
    "sqlite3 goodreads.db\n",
    "sqlite> .read goodreads.db\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4d9fba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e673b4b",
   "metadata": {},
   "source": [
    "This step will load up the data for the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a38bbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db_path = \"goodreads.db\"\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913d4a2",
   "metadata": {},
   "source": [
    "Then we can query the database using natural language, and I'll enable `verbose` mode so that you can see the chain of thoughts there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "923d6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many books achieved an average rating above 4? \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT COUNT(*) FROM goodreads WHERE avg_rating > 4;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(100,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m 100 books achieved an average rating above 4.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 100 books achieved an average rating above 4.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm = llm, database = db, verbose = True)\n",
    "db_chain.run(\"How many books achieved an average rating above 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032ae48",
   "metadata": {},
   "source": [
    "Amazing! By using `verbose` mode, you can actually obverse the SQL query being constructed to extract the data based on our question, and the output in natural language as well.\n",
    "\n",
    "Let's run the query using `pandas` to verify the result, in case any hallucination in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8a6be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(sqlite_db_path)\n",
    "query = \"SELECT COUNT(*) FROM goodreads WHERE avg_rating > 4;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b9348",
   "metadata": {},
   "source": [
    "And the result checks out! But that's not it. To query larger databases and more complex schemas, we will need to introduce the use of `agents`, which typically involves running multiple sequential queries or error recovery. You can checkout the [agent_toolkits](https://github.com/hwchase17/langchain/tree/master/langchain/agents/agent_toolkits) for more details or I'll cover this concept when introducing `Agents`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec870a",
   "metadata": {},
   "source": [
    "## Code Understanding\n",
    "\n",
    "This is probably one of the most sought after features every developer or anyone who's interested in building has been dreaming for. LangChain can parse GitHub repos and generate new code using the mechnisms demonstrated so far, including `embeddings`, `vector stores`, `conversational retriever chain` and `LLMs`.\n",
    "\n",
    "Without paying for [GitHub Copilot](https://github.com/features/copilot), why not see what you can do with LLMs first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "62d5c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2de11",
   "metadata": {},
   "source": [
    "Here we do the embeddings again in preparation for loading the project as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f810bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac27ef4",
   "metadata": {},
   "source": [
    "Scan through the whole codebase, the example here is a library called [pandas-ai](https://github.com/gventuri/pandas-ai), which is a clever wrapper around LLMs and the well-known [Pandas](https://github.com/pandas-dev/pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2a7afc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs loaded: 133\n"
     ]
    }
   ],
   "source": [
    "project = \"pandas-ai\"\n",
    "docs = []\n",
    "\n",
    "for path, dirs, files in os.walk(project):\n",
    "    for file in files:\n",
    "        try: \n",
    "            loader = TextLoader(os.path.join(path, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass\n",
    "        \n",
    "print(f\"number of docs loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8039a1",
   "metadata": {},
   "source": [
    "Now we create the embeddings and store them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fbe33bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = FAISS.from_documents(docs, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"stuff\", retriever = knowledge_base.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688642c",
   "metadata": {},
   "source": [
    "Now the fun begins, we gonna test whether LLM really understands the code and can help us code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "63786e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Google PaLM2 as the LLM for queries, you can use the `GooglePalm` class provided in the code. Here's an example of how you can use it:\n",
      "\n",
      "```\n",
      "from pandasai.llm.google_palm import GooglePalm\n",
      "\n",
      "# Replace YOUR_API_KEY with your Google Cloud API key\n",
      "llm = GooglePalm(api_key='YOUR_API_KEY', model='models/text_bison_001')\n",
      "response = llm.call(instruction='Hello', value='world')\n",
      "\n",
      "print(response)\n",
      "```\n",
      "\n",
      "You need to replace `YOUR_API_KEY` with your own Google Cloud API key. You can also change the value of the `model` parameter to use a different model if necessary.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I use Google PaLM2 as the LLM for queries?\"\n",
    "output = qa.run(query)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8372db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "from pandasai.llm.google_palm import GooglePalm\n",
      "\n",
      "llm = GooglePalm(api_key=\"your_api_key\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "query = \"Please write the code to instanciate Google PaLM2 as LLM, and only respond with code.\"\n",
    "output = qa.run(query)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e0085",
   "metadata": {},
   "source": [
    "As you can see from the output, the result is pretty staggering. LLM plus LangChain can truly understand code, explain the functionalities, and it's able to produce some sensible code as a result of that. \n",
    "\n",
    "However, this is a very small sample in terms of its coding ability, you can use it to bootstrap work, but don't rely on it to code for you! Until we reach AGI, we still need a sound logical-minded person to give directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513329d",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "By now, you probably have noticed that the output of LLMs can vary dramatically. Even with the same LLM, you can still expect different results for the same question every time you rerun. Lowering the temperature setting is one way to have more controlled and predictable output, however, that's not quite the point here.\n",
    "\n",
    "Evaluation is the mechnism to quality control the pipeline, i.e. the output of applications, and make sure it does not suffer from any regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff5ce3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79889fe6",
   "metadata": {},
   "source": [
    "We need to prepare the data just like what we did in `Question Answering Over Documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aaaea91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"hamlet.pdf\")\n",
    "doc = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "knowledge_base = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7762045",
   "metadata": {},
   "source": [
    "Then prepare a list of questions and answers as test cases, which I picked a few from [this](hamlet_qna.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "500ab916",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {\"question\" : \"Where does the play take place?\", \"answer\" : \"Denmark\"},\n",
    "    {\"question\" : \"What is the name of the castle?\", \"answer\" : \"Elsinore\"},\n",
    "    {\"question\" : \"What are the first words spoken in the play?\", \"answer\" : \"Who's there?\"},\n",
    "    {\"question\" : \"How has Ophelia died?\", \"answer\" : \"She has supposedly drowned (ambiguity surrounds her death).\"},\n",
    "    {\"question\" : \"When was Hamlet written?\", \"answer\" : \"1600-1601\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c03a7d",
   "metadata": {},
   "source": [
    "Create a `RetrievalQA chain`, the `input_key` tells the chain to look for the `question` key in the dict above, as the input prompt / query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9954e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm, \n",
    "    chain_type = \"stuff\", \n",
    "    retriever = knowledge_base.as_retriever(), \n",
    "    input_key = \"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c5fdf",
   "metadata": {},
   "source": [
    "This step is like executing the unit tests, it not only outputs the test asseration, but also the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c83124ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Where does the play take place?',\n",
       "  'answer': 'Denmark',\n",
       "  'result': ' The play takes place in Denmark.'},\n",
       " {'question': 'What is the name of the castle?',\n",
       "  'answer': 'Elsinore',\n",
       "  'result': ' The castle is Elsinore.'},\n",
       " {'question': 'What are the first words spoken in the play?',\n",
       "  'answer': \"Who's there?\",\n",
       "  'result': ' The first words spoken in the play are \"Who\\'s there?\" by Francisco.'},\n",
       " {'question': 'How has Ophelia died?',\n",
       "  'answer': 'She has supposedly drowned (ambiguity surrounds her death).',\n",
       "  'result': ' Ophelia has drowned.'},\n",
       " {'question': 'When was Hamlet written?',\n",
       "  'answer': '1600-1601',\n",
       "  'result': ' Hamlet was written by William Shakespeare in the early 1600s.'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predications = chain.apply(question_answers)\n",
    "predications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd79fb",
   "metadata": {},
   "source": [
    "Once the execution result is produced, we call on `QAEvalChain` to evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d705ddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' CORRECT'},\n",
       " {'text': ' CORRECT'},\n",
       " {'text': ' CORRECT'},\n",
       " {'text': ' CORRECT'},\n",
       " {'text': ' CORRECT'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "evaluation = eval_chain.evaluate(\n",
    "    question_answers,\n",
    "    predications,\n",
    "    question_key = \"question\",\n",
    "    answer_key = \"answer\",\n",
    "    prediction_key = \"result\"\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bbafc",
   "metadata": {},
   "source": [
    "Now the results are here, all answered correctly. Despite the exceptional outcome, I was expecting it to make some tiny mistakes, which would probably seem more human. I almost tempted to create some convoluted mind-twisting asserations, I am limited by my own creativity.\n",
    "\n",
    "Another thing you might find interesting is: having QAEvalChain marking it own homework. And I am not entirely sure how that mechnism works underneath the hood. So either you should dig into it if you are interested or take it with a pinch of salt (at least for now, until I can find a self-consistent answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70055524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
