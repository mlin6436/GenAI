{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeaaaa5f",
   "metadata": {},
   "source": [
    "To understand how LangChain works, we first need to dive into the foundational building blocks of LangChain called [Components](https://docs.langchain.com/docs/category/components), including `Schema`, `Models`, `Prompts`, `Indexes`, `Memory`, `Chains` and `Agents`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae329baf",
   "metadata": {},
   "source": [
    "Let's first load up the environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "798dd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d3c67",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "Schema is the most rudimentary way for users to interact with LLMs. There are mainly three types of Schema, which are `Text`, `Documents` and `Chat Messages`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bae23",
   "metadata": {},
   "source": [
    "The primary interface to interact with LLMs is **Text**. Some may refer to it as `text in, text out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7741c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What is the weather like today in Paris?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91258170",
   "metadata": {},
   "source": [
    "LLMs can also understand and process unstructured data, referred to as **Document** in LangChain, which will typically contain `page_content` and `metadata` by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f416441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content = \"Sample document\",\n",
    "    metadata = {'time_stamp': 1685092927}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bd1fb",
   "metadata": {},
   "source": [
    "To make more fine-grained interaction with LLMs, some models provide access to the underlying API in such a way called **Chat Messages**. It can be broken down into three roles:\n",
    "- **SystemMessage**: This message sets the context and behaviour of AI, so that AI can provide guided responses without userâ€™s aware\n",
    "- **HumanMessage**: This is the human input to AI, you can also refer to it as `prompt`, however, I'll cover this later\n",
    "- **AIMessage**: This is the answer you get from AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "370d0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_model = ChatOpenAI(temperature = 0, model = \"gpt-3.5-turbo\", openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "798746aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, I'd be happy to help! There are many beautiful beaches around the world, but here are a few suggestions:\\n\\n1. Maldives - Known for its crystal-clear waters and white sandy beaches, the Maldives is a popular destination for beach lovers.\\n\\n2. Bali, Indonesia - Bali is famous for its stunning beaches, including Kuta Beach, Seminyak Beach, and Nusa Dua Beach.\\n\\n3. Phuket, Thailand - Phuket is home to some of the most beautiful beaches in Thailand, such as Patong Beach, Kata Beach, and Karon Beach.\\n\\n4. Cancun, Mexico - Cancun is a popular destination for beach lovers, with its turquoise waters and white sandy beaches.\\n\\n5. Gold Coast, Australia - The Gold Coast is known for its beautiful beaches, such as Surfers Paradise Beach, Burleigh Heads Beach, and Coolangatta Beach.\\n\\nI hope this helps!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful chat bot that will answer questions from user. If you don't know the answer, just say that you don't know. Do not make things up.\"),\n",
    "        HumanMessage(content=\"I like beaches, recommend places I should go for my holiday?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16c9c3",
   "metadata": {},
   "source": [
    "Of course, a typical conversation does not usually end with one round but more, and you'd expect to conduct the conversation in a continuous way with the existing context. This is how you implement it with chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c1b06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are many things you can do in Maldives, such as:\\n\\n1. Snorkeling and diving to explore the beautiful coral reefs and marine life.\\n2. Relaxing on the white sandy beaches and enjoying the crystal clear waters.\\n3. Taking a sunset cruise to enjoy the stunning views of the sunset over the Indian Ocean.\\n4. Visiting local islands to experience the Maldivian culture and cuisine.\\n5. Going on a fishing trip to catch your own dinner.\\n6. Trying out water sports such as jet skiing, parasailing, and windsurfing.\\n7. Indulging in spa treatments and massages to rejuvenate your body and mind.\\n8. Taking a seaplane or helicopter tour to see the Maldives from above.\\n9. Going on a dolphin or whale watching tour to see these magnificent creatures in their natural habitat.\\n10. Enjoying a romantic dinner on the beach under the stars.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful chat bot that will answer questions from user. If you don't know the answer, just say that you don't know. Do not make things up.\"),\n",
    "        HumanMessage(content=\"I like beaches, where should I go for my holiday?\"),\n",
    "        AIMessage(content=\"You should go to Maldives\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45887444",
   "metadata": {},
   "source": [
    "Just bear in mind, as the thread goes on, it's likely you will hit the [token limit](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) of LLMs, the behaviour of LLMs will become less predictable then, and possibly going off a tangent in answering your questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79956f6",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "When talking about LLMs, which means Large Language Models, we usually mean a wide range of models, developed and trained by different institutions, you can find a list of LLMs [here](https://en.wikipedia.org/wiki/Large_language_model) or how each LLM evolved over the years. Within a LLM, there are [different models](https://platform.openai.com/docs/models/overview) that specialise in different kind of tasks and price points. \n",
    "\n",
    "In the context of LangChain, different models are being split into the following types: `LLMs`, `Chat Models` and `Text Embedding Models`.\n",
    "\n",
    "![llm evolution](llm_evolution.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68fc39",
   "metadata": {},
   "source": [
    "**Large Language Models (LLMs)** are the type of models that take a text string as input, and return a text string as output, again referring to it as `text in, text out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b73ad38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf498c1",
   "metadata": {},
   "source": [
    "The second type of model is known as **Chat Model**, which takes a list of `chat messages` as input and returns a `chat message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8acf7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"There are many things to do in the Maldives, depending on your interests. Some popular activities include:\\n\\n1. Snorkeling or diving to see the colorful coral reefs and marine life\\n2. Relaxing on the white sandy beaches and soaking up the sun\\n3. Taking a sunset cruise or a dolphin watching tour\\n4. Exploring the local culture and visiting the fishing villages\\n5. Trying out water sports like jet skiing, windsurfing, or kayaking\\n\\nYou can also indulge in various spa treatments, enjoy fine dining experiences, and take part in cultural activities like local music and dance performances. It's best to plan ahead and book activities and tours in advance to make the most of your time in the Maldives.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key = openai_api_key)\n",
    "chat_model(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful travel assistant specialized in travel planning for users. If you don't know the answer, just say that you don't know. Do not make things up.\"),\n",
    "        HumanMessage(content=\"I like to go to Maldives, what should I do when I get there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20103a46",
   "metadata": {},
   "source": [
    "The third type of model is **Text Embedding Model**, it takes a series of text as input and returns a vector representation of text. This sounds very abstract but a very powerful concept, you can head straight to the next part of this tutorial to checkout its amazing capabilities. But to give you a quick summary, the vector values are used to compare closely related are two pieces of text, and that can be used to extract results closest in answering a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "855f32f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text embeddings result: [-0.011748258024454117, 0.007892419584095478, -0.023871390148997307, -0.015503684990108013, -0.005690039601176977, 0.018261680379509926, -0.010677192360162735, -0.011741564609110355, -0.014097910374403, -0.024580972269177437]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "text = \"this is a line of text\"\n",
    "text_embeddings = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"Sample text embeddings result: {text_embeddings[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede960d",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "A `prompt` refers to the input to the LLM. A prompt can be simply a string of text that gets passed into LLM, or it can be constructed using `PromptTemplate` to make it more extensible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf4c26",
   "metadata": {},
   "source": [
    "By far, you would have seen numerous examples like this, which is passing **prompt value** straight into LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8ed7128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTomorrow is Saturday.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "promptValue = \"Today is Friday, what day is it tomorrow?\"\n",
    "\n",
    "llm(promptValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da559ee2",
   "metadata": {},
   "source": [
    "Now let's take a look at how we can build a more structured object using **PromptTemplate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c443ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I like to travel to Paris, what should I do when I get there?\n",
      "Answer: \n",
      "\n",
      "1. Visit the Eiffel Tower.\n",
      "2. Take a Seine River cruise.\n",
      "3. Visit the Louvre.\n",
      "4. Take a stroll through the Jardin des Tuileries.\n",
      "5. Explore the Latin Quarter.\n",
      "6. Check out the Catacombs of Paris.\n",
      "7. Visit the Notre Dame Cathedral.\n",
      "8. Enjoy a picnic in the Luxembourg Gardens.\n",
      "9. Shop along the Champs-Ã‰lysÃ©es.\n",
      "10. Taste the local cuisine.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "\n",
    "template = \"\"\"I like to travel to {location}, what should I do when I get there?\"\"\"\n",
    "promptTemplate = PromptTemplate(\n",
    "    input_variables = [\"location\"],\n",
    "    template = template\n",
    ")\n",
    "prompt = promptTemplate.format(location = \"Paris\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "answer = llm(prompt)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5c41d",
   "metadata": {},
   "source": [
    "Before diving into using **Example Selectors**. Let's step back and look at a number of prompting techniques:\n",
    "- **Zero-shot prompting**: The model is given no training data on the task at all. Instead, the prompt provides the model with a general description of the task and asks the model to generate a response.\n",
    "- **One-shot prompting**: The model is given a single training example on the task. The example is typically a short text passage that describes the task and provides the correct answer.\n",
    "- **Few-shot prompting**: The model is given a small number of training examples on the task. The number of examples is typically between two and five.\n",
    "\n",
    "**Example Selectors** is how you can apply the few-shot prompting technique to get better and more accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d983b78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the the classification of items\n",
      "\n",
      "Input: apple\n",
      "Output: fruit\n",
      "\n",
      "Input: orange\n",
      "Output: fruit\n",
      "\n",
      "Input: kiwi\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"apple\", \"output\": \"fruit\"},\n",
    "    {\"input\": \"orange\", \"output\": \"fruit\"},\n",
    "    {\"input\": \"monkey\", \"output\": \"mammal\"},\n",
    "    {\"input\": \"beetle\", \"output\": \"insect\"},\n",
    "    {\"input\": \"tree\", \"output\": \"plant\"},\n",
    "]\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, \n",
    "    OpenAIEmbeddings(openai_api_key = openai_api_key), \n",
    "    FAISS, \n",
    "    k=2\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix=\"Give the the classification of items\",\n",
    "    suffix=\"Input: {item}\\nOutput:\",\n",
    "    input_variables=[\"item\"],\n",
    ")\n",
    "input = \"kiwi\"\n",
    "print(similar_prompt.format(item = input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f35ec9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fruit'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(item = input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac151251",
   "metadata": {},
   "source": [
    "When users enter prompts, they expect some form of output in return. However, this is largely down to LLMs' discretion on how they decide to display the answer. In order to get more structured output, say in the form of `json`, you can include a **Output Parser** as part of prompt input to give LLMs direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f88026c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"unformatted output\": string  // output without any formatting\n",
      "\t\"formatted output\": string  // output being formatted by output parser\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema \n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name = \"unformatted output\", description = \"output without any formatting\"),\n",
    "    ResponseSchema(name = \"formatted output\", description = \"output being formatted by output parser\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3854241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reformat the user input using the instructions below and correct the spellng.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"unformatted output\": string  // output without any formatting\n",
      "\t\"formatted output\": string  // output being formatted by output parser\n",
      "}\n",
      "```\n",
      "\n",
      "Input:\n",
      "   why did the chiken cros the raod? Toget to the other side.\n",
      "\n",
      "Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Reformat the user input using the instructions below and correct the spellng.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Input:\n",
    "{input}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "promptTemplate = PromptTemplate(\n",
    "    input_variables = [\"input\"],\n",
    "    partial_variables = {\"format_instructions\": format_instructions},\n",
    "    template = template\n",
    ")\n",
    "prompt = promptTemplate.format(input = \"   why did the chiken cros the raod? Toget to the other side.\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d2d7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"unformatted output\": \"why did the chiken cros the raod? Toget to the other side.\",\n",
      "\t\"formatted output\": \"Why did the chicken cross the road? To get to the other side.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5dcf57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unformatted output': 'why did the chiken cros the raod? Toget to the other side.',\n",
       " 'formatted output': 'Why did the chicken cross the road? To get to the other side.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37393e0c",
   "metadata": {},
   "source": [
    "## Indexes\n",
    "\n",
    "A word, such as \"apple\" is more than just a string, it contains multi-dimensional information, such as smell, taste, shape, colour etc. This is essential to how LLMs understand and interact with information. The way to represent the rich information a word contains is called [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics). This vector value represents the statistical relationship between two words in the dataset. \n",
    "\n",
    "Indexes are the efficient ways for LLMs to search through the vector values and generate content based on the datasets they are trained on. \n",
    "\n",
    "Don't worry if it still sounds a bit abstract, let's walk through a sequence of steps to familiarise you with what is involved and how it works, which are `Document Loaders`, `Text Splitters`, `VectorStores` and `Retrievers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bbbd63",
   "metadata": {},
   "source": [
    "**Document Loaders**, as it says on the tin, are the mechanisms to transform a range of content from various sources into `Document` format, which was introduced earlier. \n",
    "\n",
    "The data source can be:\n",
    "- Different types of static content, including text, html, json, pdf, powerpoint, images, etc.\n",
    "- Public dataset or service, like Wikipedia, Hacker News, YouTube transcripts. etc.\n",
    "- Proprietary dataset or service, such as AWS S3, Confluence, Git, etc.\n",
    "\n",
    "The [list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) goes on. You get the idea, there is probably a Document Loader for the type of content you had in mind. \n",
    "\n",
    "Let's see how it works in action by loading up the play `Hamlet` in PDF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad3c85a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages in the doc: 142\n",
      "number of characters in the doc: 179843\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"hamlet.pdf\")\n",
    "doc = loader.load()\n",
    "\n",
    "print(f\"number of pages in the doc: {len(doc)}\")\n",
    "total_chars = sum([len(page.page_content) for page in doc])\n",
    "print(f\"number of characters in the doc: {total_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a1476",
   "metadata": {},
   "source": [
    "Most LLMs are constrained by the number of `tokens` you can pass in, and it's worth emphasising a token does not equal a word. You can experiment how tokenisation works using [this](https://platform.openai.com/tokenizer). But what does that have to do with splitting text? \n",
    "\n",
    "The short answer is: you are likely to be dealing with long pieces of text, which can easily surpass the limit of number of tokens allowed by LLMs. And there's more to it: how do you keep the `semantically related` text together so that the splitting does not interrupt or change the meaning of text.\n",
    "\n",
    "**Text Splitters** introduces the concept of `chunk size` and `chunk overlap`. Chunk size is like a `sliding window` that is used to determine the length of a particular chunk of content, it is measured by the number of characters and this value needs to be less than the max tokens allowed by LLMs. Chunk overlap is how many characters the current chunk should have crossed over with the previous one.\n",
    "\n",
    "Even though text splitter may cover majority of use cases, you may want to use specific types of [splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1466d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 270\n",
      "sample of split file: \n",
      "HORATIO\n",
      "That can I;\n",
      "At least, the whisper goes so. Our last king,\n",
      "Whose image even but now appear'd to us,\n",
      "Was, as you know, by Fortinbras of Norway,\n",
      "Thereto prick'd on by a most emulate pride,\n",
      "Dared to the combat; in which our valiant Hamlet--\n",
      "For so this side of our known world esteem'd him--\n",
      "Did slay this Fortinbras; who by a seal'd compact,\n",
      "Well ratified by law and heraldry,\n",
      "Did forfeit, with his life, all those his lands\n",
      "Which he stood seized of, to the conqueror:\n",
      "Against the which, a moiety competent\n",
      "Was gaged by our king; which had return'd\n",
      "To the inheritance of Fortinbras,\n",
      "Had he been vanquisher; as, by the same covenant,\n",
      "And carriage of the article design'd,\n",
      "His fell to Hamlet. Now, sir, young Fortinbras,\n",
      "Of unimproved mettle hot and full,\n",
      "Hath in the skirts of Norway here and there\n",
      "Shark'd up a list of lawless resolutes,\n",
      "For food and diet, to some enterprise\n",
      "That hath a stomach in't; which is no other--\n",
      "As it doth well appear unto our state--\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    ")\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "print(f\"number of documents: {len(docs)}\")\n",
    "print (f\"sample of split file: \\n{docs[10].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366a0a7",
   "metadata": {},
   "source": [
    "**Vectorstore** is created and optimised to host the vector values created via [Embeddings](https://platform.openai.com/docs/guides/embeddings) and perform similarity search. Don't get put off by the fancy words, we are still talking about searching for a result in a database. The only difference is: the source is the blob of text, the query is now natural language, and vectorstore and embeddings are here to bridge that gap.\n",
    "\n",
    "Some popular vector store implementations including local choices [FAISS](https://github.com/facebookresearch/faiss) and [Chroma](https://www.trychroma.com/) and SaaS options [Pinecone](https://www.pinecone.io/), [Weaviate](https://weaviate.io/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f63a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='SCENE Denmark.', metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 3, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content=\"Act IV\\nScene 1\\nA room in the castle.\\nEnter KING CLAUDIUS, QUEEN GERTRUDE, ROSENCRANTZ, and GUILDENSTERN\\nKING CLAUDIUS\\nThere's matter in these sighs, these profound heaves:\\nYou must translate: 'tis fit we understand them.\\nWhere is your son?\\nQUEEN GERTRUDE\\nBestow this place on us a little while.\\nExeunt ROSENCRANTZ and GUILDENSTERN\\nAh, my good lord, what have I seen to-night!\\nKING CLAUDIUS\\nWhat, Gertrude? How does Hamlet?\\nQUEEN GERTRUDE\\nMad as the sea and wind, when both contend\\nWhich is the mightier: in his lawless fit,\\nBehind the arras hearing something stir,\\nWhips out his rapier, cries, 'A rat, a rat!'\\nAnd, in this brainish apprehension, kills\\nThe unseen good old man.\\nKING CLAUDIUS\\nO heavy deed!\\nIt had been so with us, had we been there:\\nHis liberty is full of threats to all;\\nTo you yourself, to us, to every one.\\nAlas, how shall this bloody deed be answer'd?\\nIt will be laid to us, whose providence\\nShould have kept short, restrain'd and out of haunt,\", metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 91, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content=\"You from the Polack wars, and you from England,\\nAre here arrived give order that these bodies\\nHigh on a stage be placed to the view;\\nAnd let me speak to the yet unknowing world\\nHow these things came about: so shall you hear\\nOf carnal, bloody, and unnatural acts,\\nOf accidental judgments, casual slaughters,\\nOf deaths put on by cunning and forced cause,\\nAnd, in this upshot, purposes mistook\\nFall'n on the inventors' reads: all this can I\\nTruly deliver.\\nPRINCE FORTINBRAS\\nLet us haste to hear it,\\nAnd call the noblest to the audience.\\nFor me, with sorrow I embrace my fortune:\\nI have some rights of memory in this kingdom,\\nWhich now to claim my vantage doth invite me.\\nHORATIO\\nHAMLET - Act V\\n141\", metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 140, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''})]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "query = \"Where does the play take place?\"\n",
    "top_3_matches = vector_store.similarity_search(query = query, k = 3)\n",
    "\n",
    "print(top_3_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18844d7a",
   "metadata": {},
   "source": [
    "Instead of doing `similarity_search` as above, you can also use a generic interface called **Retriever** that makes it easy to combine documents with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aff9514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='SCENE Denmark.', metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 3, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content=\"Act IV\\nScene 1\\nA room in the castle.\\nEnter KING CLAUDIUS, QUEEN GERTRUDE, ROSENCRANTZ, and GUILDENSTERN\\nKING CLAUDIUS\\nThere's matter in these sighs, these profound heaves:\\nYou must translate: 'tis fit we understand them.\\nWhere is your son?\\nQUEEN GERTRUDE\\nBestow this place on us a little while.\\nExeunt ROSENCRANTZ and GUILDENSTERN\\nAh, my good lord, what have I seen to-night!\\nKING CLAUDIUS\\nWhat, Gertrude? How does Hamlet?\\nQUEEN GERTRUDE\\nMad as the sea and wind, when both contend\\nWhich is the mightier: in his lawless fit,\\nBehind the arras hearing something stir,\\nWhips out his rapier, cries, 'A rat, a rat!'\\nAnd, in this brainish apprehension, kills\\nThe unseen good old man.\\nKING CLAUDIUS\\nO heavy deed!\\nIt had been so with us, had we been there:\\nHis liberty is full of threats to all;\\nTo you yourself, to us, to every one.\\nAlas, how shall this bloody deed be answer'd?\\nIt will be laid to us, whose providence\\nShould have kept short, restrain'd and out of haunt,\", metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 91, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content=\"You from the Polack wars, and you from England,\\nAre here arrived give order that these bodies\\nHigh on a stage be placed to the view;\\nAnd let me speak to the yet unknowing world\\nHow these things came about: so shall you hear\\nOf carnal, bloody, and unnatural acts,\\nOf accidental judgments, casual slaughters,\\nOf deaths put on by cunning and forced cause,\\nAnd, in this upshot, purposes mistook\\nFall'n on the inventors' reads: all this can I\\nTruly deliver.\\nPRINCE FORTINBRAS\\nLet us haste to hear it,\\nAnd call the noblest to the audience.\\nFor me, with sorrow I embrace my fortune:\\nI have some rights of memory in this kingdom,\\nWhich now to claim my vantage doth invite me.\\nHORATIO\\nHAMLET - Act V\\n141\", metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 140, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content=\"there. Be the players ready?\\nROSENCRANTZ\\nAy, my lord; they stay upon your patience.\\nQUEEN GERTRUDE\\nCome hither, my dear Hamlet, sit by me.\\nHAMLET\\nNo, good mother, here's metal more attractive.\\nLORD POLONIUS\\nTo KING CLAUDIUS\\nO, ho! do you mark that?\\nHAMLET - Act III\\n70\", metadata={'source': 'hamlet.pdf', 'file_path': 'hamlet.pdf', 'page': 69, 'total_pages': 142, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'FOP 0.19.0-CVS', 'creationDate': '', 'modDate': '', 'trapped': ''})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "relevant_documents = retriever.get_relevant_documents(\"Where does the play take place?\")\n",
    "\n",
    "print(relevant_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1a5e9",
   "metadata": {},
   "source": [
    "Nice work for making this far, this is one of the most disorientation section looking retrospectively. Moving ahead, I will show you how to use the results generated from the steps in Indexes with a very important concept, called `Chains`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18e543",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "At the heart of LangChain is the concept of `Chains`. It creates a sequence of modular components combined in a particular way to accomplish a common goal. We could be here all day if I were to go through every single type of chain. Instead, I will focus on continuing the train of thought from `Indexes` by showcasing a couple of chains, you can look up in the [guide](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) for a more comprehensive list of functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079412ce",
   "metadata": {},
   "source": [
    "From the previous section, retriever has returned the top results for the question \"Where does the play take place?\". However, it has really given at an answer yet. Let's look at how we can do that with a chain called `RetrievalQA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83216e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe play takes place in Denmark, specifically in a room in the castle in Act IV, Scene 1, as well as in the kingdom in Act V, Scene 1. In Act III, Scene 1, the characters are in the castle, in the presence of King Claudius.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = \"refine\", retriever = retriever)\n",
    "query = \"Where does the play take place?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3ec89",
   "metadata": {},
   "source": [
    "Sweet, there is our answer. The play indeed took play in Denmark. \n",
    "\n",
    "Further more to that, we can chain a number of chains together to complete a more complex task. Now that we have some selected parts of the play, let's try to summarise those documents to create a synopsis, and produce a review piece from the synopsis, just like a professional play critic.\n",
    "\n",
    "First let's create a `summarize chain` and observe that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6764b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"SCENE Denmark.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Act IV\n",
      "Scene 1\n",
      "A room in the castle.\n",
      "Enter KING CLAUDIUS, QUEEN GERTRUDE, ROSENCRANTZ, and GUILDENSTERN\n",
      "KING CLAUDIUS\n",
      "There's matter in these sighs, these profound heaves:\n",
      "You must translate: 'tis fit we understand them.\n",
      "Where is your son?\n",
      "QUEEN GERTRUDE\n",
      "Bestow this place on us a little while.\n",
      "Exeunt ROSENCRANTZ and GUILDENSTERN\n",
      "Ah, my good lord, what have I seen to-night!\n",
      "KING CLAUDIUS\n",
      "What, Gertrude? How does Hamlet?\n",
      "QUEEN GERTRUDE\n",
      "Mad as the sea and wind, when both contend\n",
      "Which is the mightier: in his lawless fit,\n",
      "Behind the arras hearing something stir,\n",
      "Whips out his rapier, cries, 'A rat, a rat!'\n",
      "And, in this brainish apprehension, kills\n",
      "The unseen good old man.\n",
      "KING CLAUDIUS\n",
      "O heavy deed!\n",
      "It had been so with us, had we been there:\n",
      "His liberty is full of threats to all;\n",
      "To you yourself, to us, to every one.\n",
      "Alas, how shall this bloody deed be answer'd?\n",
      "It will be laid to us, whose providence\n",
      "Should have kept short, restrain'd and out of haunt,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"You from the Polack wars, and you from England,\n",
      "Are here arrived give order that these bodies\n",
      "High on a stage be placed to the view;\n",
      "And let me speak to the yet unknowing world\n",
      "How these things came about: so shall you hear\n",
      "Of carnal, bloody, and unnatural acts,\n",
      "Of accidental judgments, casual slaughters,\n",
      "Of deaths put on by cunning and forced cause,\n",
      "And, in this upshot, purposes mistook\n",
      "Fall'n on the inventors' reads: all this can I\n",
      "Truly deliver.\n",
      "PRINCE FORTINBRAS\n",
      "Let us haste to hear it,\n",
      "And call the noblest to the audience.\n",
      "For me, with sorrow I embrace my fortune:\n",
      "I have some rights of memory in this kingdom,\n",
      "Which now to claim my vantage doth invite me.\n",
      "HORATIO\n",
      "HAMLET - Act V\n",
      "141\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"there. Be the players ready?\n",
      "ROSENCRANTZ\n",
      "Ay, my lord; they stay upon your patience.\n",
      "QUEEN GERTRUDE\n",
      "Come hither, my dear Hamlet, sit by me.\n",
      "HAMLET\n",
      "No, good mother, here's metal more attractive.\n",
      "LORD POLONIUS\n",
      "To KING CLAUDIUS\n",
      "O, ho! do you mark that?\n",
      "HAMLET - Act III\n",
      "70\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\n",
      "This scene takes place in Denmark.\n",
      "\n",
      "\n",
      "King Claudius and Queen Gertrude question Rosencrantz and Guildenstern about Hamlet's whereabouts. Once they're alone, Queen Gertrude reveals that Hamlet is mad and killed an unseen old man in his fit of rage. King Claudius is horrified by the news, and worries that the deed will be laid to them for not restraining Hamlet.\n",
      "\n",
      " Prince Fortinbras orders that the bodies of those killed in the Polack and English wars be put on display for the world to see, so that he can explain how these things happened and how the inventors' purposes were mistaken. He invites the noblest to the audience to hear the story, and claims his rights of memory in the kingdom.\n",
      "\n",
      "\n",
      "Hamlet and the other characters discuss if the players are ready. Gertrude invites Hamlet to sit with her, but he declines, saying there is something more attractive. Lord Polonius remarks on this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" King Claudius and Queen Gertrude question Rosencrantz and Guildenstern about Hamlet's whereabouts, and Queen Gertrude reveals that Hamlet is mad and killed an old man. Prince Fortinbras orders the bodies of those killed in the Polack and English wars to be put on display and invites the noblest to the audience. Hamlet and the other characters discuss if the players are ready, and Gertrude invites him to sit with her but he declines.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(temperature = 0.7, openai_api_key = openai_api_key)\n",
    "summarize_chain = load_summarize_chain(llm, chain_type = \"map_reduce\", verbose = True)\n",
    "summarize_chain.run(relevant_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5b04c",
   "metadata": {},
   "source": [
    "You can see from the trace, `summarize_chain` would start a number of chains to perform the sub-tasks, `MapReduceDocumentsChain` and `StuffDocumentsChain`. For each chain, it created subsequent `LLMChain`, which performs the \"text in, text out\" operation to finish the sub-task before return the result to the parent chain.\n",
    "\n",
    "This is nice, and we can add more to it. In the following example, you will see how a number of chains are being connected by `SimpleSequentialChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b7c73839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature = 0.7, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058a588",
   "metadata": {},
   "source": [
    "Firstly, we will define a `summarize_chain`, it looks slightly different from the previous definition, because we will be taking input from `SimpleSequentialChain` with a custom prompt, and output result `synopsis` to be consumed by the next chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d758f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"text\"])\n",
    "summarize_chain = load_summarize_chain(llm, chain_type = \"stuff\", prompt = prompt, output_key = \"synopsis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aae11",
   "metadata": {},
   "source": [
    "Secondly, we create a simple `LLMChain` that will carry out the task of creating a review from the synopsis generated by the `summarize_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "787eb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt = PromptTemplate(input_variables = [\"synopsis\"], template = template)\n",
    "review_chain = LLMChain(llm = llm, prompt = prompt, output_key = \"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6318d",
   "metadata": {},
   "source": [
    "Lastly, we provide both `summarize_chain` and `review_chain` to `SimpleSequentialChain` that will delegate work to each chain respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "66c38e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m King Claudius, Queen Gertrude, Rosencrantz and Guildenstern are in a room in the castle. Queen Gertrude expresses her shock at the recent events and wonders how Hamlet is, to which King Claudius replies. He expresses his fear of the consequences of Hamlet's actions and decides to tell the world what happened to avoid blame. Prince Fortinbras arrives and expresses his claim to the kingdom. They all prepare for the play, and Hamlet refuses to sit by his mother. Lord Polonius wonders what Hamlet means by his words.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "The latest production of \"Hamlet\" is a captivating, gripping experience that will leave theatergoers enthralled throughout its entirety. Refreshingly original and thought-provoking, this play is a must-see for anyone with an appreciation for classic theater. \n",
      "\n",
      "This adaptation of the classic Shakespearean tragedy managed to breathe new life into the centuries-old work and bring it to a modern audience. The actors were able to deliver a powerful performance that was affecting and raw, as they navigated the complicated emotions and relationships between the characters. \n",
      "\n",
      "The set was simple yet effective, creating an atmosphere of tension and drama that was crucial to the story. The costumes were also tasteful and well-chosen, evoking the time period of the play without being too elaborate. \n",
      "\n",
      "Ultimately, this production of \"Hamlet\" is an absolute success. It is sure to leave a lasting impact on its viewers and will remain an unforgettable experience for years to come.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe latest production of \"Hamlet\" is a captivating, gripping experience that will leave theatergoers enthralled throughout its entirety. Refreshingly original and thought-provoking, this play is a must-see for anyone with an appreciation for classic theater. \\n\\nThis adaptation of the classic Shakespearean tragedy managed to breathe new life into the centuries-old work and bring it to a modern audience. The actors were able to deliver a powerful performance that was affecting and raw, as they navigated the complicated emotions and relationships between the characters. \\n\\nThe set was simple yet effective, creating an atmosphere of tension and drama that was crucial to the story. The costumes were also tasteful and well-chosen, evoking the time period of the play without being too elaborate. \\n\\nUltimately, this production of \"Hamlet\" is an absolute success. It is sure to leave a lasting impact on its viewers and will remain an unforgettable experience for years to come.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains = [summarize_chain, review_chain], verbose = True)\n",
    "overall_chain.run(relevant_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b001d3",
   "metadata": {},
   "source": [
    "If you ignore the \"creativity\" in the review, it's a really cool application of LLMs. By using chain, we can actually provide a short term or even long term memory to LLMs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64ee4b",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "In essence, memory keeps track of state information between user input and LLMs output. Memory works in many ways. The simplest way is to create a `ConversationBufferMemory` and add messages to it. Alternatively, it's easier to use `ConversationBufferMemory` in combination with `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "88a6e4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Ophelia is a character in William Shakespeare's play Hamlet. She is the daughter of Polonius and the potential wife of Prince Hamlet.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature = 0.7, openai_api_key = openai_api_key)\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vector_store.as_retriever(), memory = memory)\n",
    "qa.run(\"Who is Ophelia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7557a5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ophelia drowned in a river.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"How has she died?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8409711",
   "metadata": {},
   "source": [
    "From this example, you can see LLMs can use memory to infer the previous context to give more targeted answer. Let's also take a look at what's in memory as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9c797eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Who is Ophelia?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Ophelia is a character in William Shakespeare's play Hamlet. She is a young noblewoman of Denmark, the daughter of Polonius, sister of Laertes, and potential wife of Prince Hamlet.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How has she died?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Ophelia died by drowning.', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf11a4f",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Agent is one the most powerful concept in LangChian, it basically frees users up from explicitly specifying the sequence of actions to complete a task, but delegating the decision making process to `agents` using a set of `tools` in their `toolkit`. [Tools](https://python.langchain.com/en/latest/modules/agents/tools.html) are specific programs or scripts that can be used to perform a specific task, such as google search, making web requests. [Toolkit](https://python.langchain.com/en/latest/modules/agents/toolkits.html), on the other hand, is a collection of tools designed to be used together.\n",
    "\n",
    "On top of that, `Agent Executor` is at the centre of planning and executing. It is responsible for orchestrating the agent and the tools to determine which tools to call and in what order.\n",
    "\n",
    "Let's take a look at how these concepts glue together with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0823b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "llm = OpenAI(temperature = 0, openai_api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0bbe7",
   "metadata": {},
   "source": [
    "We create a vector store that stores the play `Hamlet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6285da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"hamlet.pdf\")\n",
    "doc = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    ")\n",
    "docs = text_splitter.split_documents(doc)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "hamlet = Chroma.from_documents(docs, embeddings, collection_name = \"hamlet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009d296",
   "metadata": {},
   "source": [
    "Similarly, we create a vector store to store the play `King Lear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e0cc9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(\"king_lear.pdf\")\n",
    "doc = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    ")\n",
    "docs = text_splitter.split_documents(doc)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)\n",
    "king_lear = Chroma.from_documents(docs, embeddings, collection_name = \"king_lear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce195412",
   "metadata": {},
   "source": [
    "Now, we will load both plays into `agent executor`, where the decision of which book to look up for relevant questions  is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "55e89955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import VectorStoreInfo, VectorStoreRouterToolkit, create_vectorstore_router_agent\n",
    "\n",
    "hamlet_vectorstore_info = VectorStoreInfo(\n",
    "    name = \"hamlet\",\n",
    "    description = \"Information about Hamlet\",\n",
    "    vectorstore = hamlet\n",
    ")\n",
    "king_lear_vectorstore_info = VectorStoreInfo(\n",
    "    name = \"king lear\",\n",
    "    description = \"Information about King Lear\",\n",
    "    vectorstore = king_lear\n",
    ")\n",
    "toolkit = VectorStoreRouterToolkit(\n",
    "    vectorstores = [hamlet_vectorstore_info, king_lear_vectorstore_info],\n",
    "    llm = llm\n",
    ")\n",
    "agent_executor = create_vectorstore_router_agent(\n",
    "    llm = llm,\n",
    "    toolkit = toolkit,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa3ca5",
   "metadata": {},
   "source": [
    "The question gives a clear hint that the agent executor should look into `Hamlet` for answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6d07968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find information about Hamlet\n",
      "Action: hamlet\n",
      "Action Input: Who is Prince Hamlet?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Prince Hamlet is the protagonist of William Shakespeare's play The Tragedy of Hamlet, Prince of Denmark. He is the son of the late King Hamlet and the nephew of the current King Claudius.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Prince Hamlet is the protagonist of William Shakespeare's play The Tragedy of Hamlet, Prince of Denmark. He is the son of the late King Hamlet and the nephew of the current King Claudius.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Prince Hamlet is the protagonist of William Shakespeare's play The Tragedy of Hamlet, Prince of Denmark. He is the son of the late King Hamlet and the nephew of the current King Claudius.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Who is Prince Hamlet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f83061",
   "metadata": {},
   "source": [
    "The following question is more subtle, `Edmund` is a character in `King Lear`. Therefore, the agent executor will need to determine which book to use and provide an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e386db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Edmund is.\n",
      "Action: king lear\n",
      "Action Input: Who is Edmund?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m Edmund is a character in the play King Lear by William Shakespeare. He is the illegitimate son of the Earl of Gloucester.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Edmund is a character in the play King Lear by William Shakespeare. He is the illegitimate son of the Earl of Gloucester.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Edmund is a character in the play King Lear by William Shakespeare. He is the illegitimate son of the Earl of Gloucester.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Who is Edmund?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36df950",
   "metadata": {},
   "source": [
    "There are also many other types of [agents](https://python.langchain.com/en/latest/modules/agents/agents.html) and use cases for [agent executor](https://python.langchain.com/en/latest/modules/agents/agent_executors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdad49",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "Congratulations! We have reached the end of this exercise. To recap, the tutorial introduces the basic components that make up LangChain. We have experimented `schema`, `models` and `prompts`, and studies how to make sure of `indexes`, `chains` and `memory` to build functioning products that solve context issues with LLMs and extend their capabilities. Finally, we arrived at `agents` where we explored semi-automated workloads and I hope that inspires you to build!\n",
    "\n",
    "If you are keen to learn more, go straight to `Use Cases` where I will walk you through the use cases of LangChain with detailed examples to showcase its amazing capabilities.\n",
    "\n",
    "Checkout other learnings and resources I shared in my [GitHub](https://github.com/mlin6436/GenAI).\n",
    "\n",
    "If you'd like to share your questions and feedback, or keen to get involved in building, you can reach out to me directly on [Twitter](https://twitter.com/mlin6436) and [LinkedIn](https://www.linkedin.com/in/mlin6436/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
